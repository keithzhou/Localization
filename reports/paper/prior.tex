\chapter{Background}

\section{Point localization}
Acoustic localization has been researched extensively in the literature. Localization techniques can be broadly categorized into Interaural Level Difference (ILD), Location Template Matching (LTM), and Time Difference of Arrival (TDOA) based approaches.

\subsection{ILD}
ILD techniques rely on the observation that signal intensity decays as the distance to the microphone increases. A microphone closer to the signal source would receive the signal with higher intensity than a microphone farther away. With multiple microphones, it is possible to infer the source location by comparing the signal intensity received at different microphones. Human's auditory system has used ILD cues to infer source direction~\cite{ild:2}, and this technique is most effective to localize high frequency sources, because they don't diffract effectively around the listener's head and produce a significant intensity difference.

For a point sound source in a direct field, the signal intensity decays proportional to the square of the distance between the source and the microphone. Let $I_i$ denote the received signal intensity at microphone $i$: 
\begin{eqnarray}\label{eqn:ild1}
I_i \propto \frac{1}{d_i^2} I_s
\end{eqnarray}
where $d_i$ is the distance between the audio source and microphone $i$, and $I_s$ is the sound intensity at the source.
With two microphones, the signal intensity received at both microphones has to satisfy equation~\ref{eqn:ild1}:
\begin{eqnarray}
I_1d_1^2 &=& I_2d_2^2\\
\label{eqn:ild}
\frac{I_1}{I_2} &=& \frac{d_2^2}{d_1^2}
\end{eqnarray}
It can be shown that, on a 2D plane, points satisfy equation~\ref{eqn:ild} form a circle when $I_1 \ne I_2$ and form a line when $I_1 = I_2$~\cite{ild:1}. With two microphones all points on this curve generate the same intensity ratio and can not be distinguished from each other. Multiple approaches have been investigated to eliminate this ambiguity. \cite{ild:1} employed multiple microphones and used the intersection of circles from each microphone pair to estimate the source location. \cite{ild:3} combined ILD with Interaural Time Difference (ITD) to estimate the source direction (i.e azimuth). Instead of solving the intersection of circles, \cite{ild:4} employed machine learning techniques to automatically learn the mapping from ILD and ITD features to location coordinate. It uses four microphones and requires a training phase, during which the sound source is manually placed at predetermined locations for the system to learn the parameters that map from feature space to the sound location.

ILD approach relies on the accurate measurement of the received intensity ratio between microphone pairs. Any obstacle object between the sound source and any microphone would produce a significant distortion in the measured intensity ratio. We find this approach to be too restrictive for our system, since in an interactive system we can not control whether or not the user places any obstacle between the sound source and the microphones.

\subsection{LTM}
In LTM based approaches, acoustic templates acquired from different locations are first stored in the system during a ``training'' phase. Localization can be performed by comparing the incoming waveform with the stored templates, and the location with the best matching template is chosen as the output. Authors in \cite{extended:tusi} has built a virtual piano keyboard application where different musical notes are associated with different locations on a surface. A user can then play by tapping at different locations on the surface. Different ways of extracting templates from raw acoustic source and different similarity measures have been investigated in the past. 

\cite{extended:tusi} and \cite{ltm:pham} investigated using max value from cross-correlation as a similarity measure to localize user taps on interactive surfaces. \cite{ltm:lpc} used L2 distance in the Linear Predictive Coding coefficient space as a similarity measure to localize taps on surfaces. \cite{ltm:tusi2} further explored accuracy improvement by using multiple templates for each location and speed improvement by merging multiple templates into one representative template.

The requirement of having a template for each location to be detected makes this approach too restrictive for our project, since we want the localization to be continuous in a 2D region. \cite{ltm:tusi2} has investigated contiguous localization by linearly extrapolating between stored locations, but result has high error.  Moreover, the need to recalibrate all locations during setup is too cumbersome for the end users in a portable system. Therefore, our main focus will be on TDOA based approaches.

\subsection{TDOA}
TDOA approaches exploit the difference of arrival time between the acoustic source and two fixed microphones on the plane. It can be easily shown that the acoustic sources with the same TDOA to two fixed microphones on the plane form a hyperbola. When you have more than two microphones, each pair would give a different hyperbola. The intersection of all the hyperbolas marks the source location. TDOA approaches rely on accurate estimates of arrival time differences between microphones. 

In \cite{tdoa:ppp}, authors used eight microphones mounted on the corners of a ping pong table to localize points where the ball hits the table. They used a threshold to determine the arrival time of acoustic signal. This approach works well in noise free environment but the performance degrades with background noise. Their approach also suffers from dispersive deflections that arrive before the main wavefront of the acoustic signal. To make it more robust, authors in \cite{tdoa:mit3} and \cite{tdoa:mit4} extracted descriptive parameters for each significant peak(e.g., peak height, width, mean arrival time). The algorithm then used extracted parameters to predict arrival time with a second order polynomial, the parameters of which were fitted during calibration at fixed locations.  Authors in \cite{tdoa:mit5} have used similar techniques and built an interactive window by placing four microphones on four corners of a glass pane. The glass window was installed in shopping centers. A projector projects product information onto the glass where consumers can browse between pages by tapping on the window.

Cross-correlation has also been used to measure signal arrival time differences\cite{tdoa:mit2, tdoa:micloc, tdoa:3}.  Cross correlation is a measure of similarity between two signals. For real valued signals $x_1(t)$ and $x_2(t)$, the cross-correlation between them at a particular time shift $\tau$ can be calculated as:
\begin{eqnarray}\label{eqn:gcc0}
 R_{x_1,x_2}(\tau) = \int_{-\infty}^{\infty} x_1(t) x_2(t+\tau)dt
\end{eqnarray}
We can take Fourier Transform on both sides of equation~\ref{eqn:gcc0}:
\begin{eqnarray}\label{eqn:gcc1}
\mathcal{F}\{ R_{x_1,x_2}(\tau)\} & = & X_1(\omega) X_2(-\omega)\\
& =& X_1(\omega) X_2(\omega)^*
\end{eqnarray}
Where $X_1(\omega)$ and $X_2(\omega)$ are the Fourier Transforms of $x_1(t)$ and $x_2(t)$. We can retrieve the cross-correlation result in time domain by taking inverse Fourier transform:
\begin{eqnarray}\label{eqn:gcc2}
 R_{x_1,x_2}(\tau) = \int_{-\infty}^\infty X_1(\omega)X_2(\omega)^*e^{j\omega\tau} d\omega
\end{eqnarray}
The arrival time difference $t_0$ is the time shift $\tau$ that maximizes~\ref{eqn:gcc2}:
\begin{eqnarray}\label{eqn:gcc3}
t_0 = \arg\max_\tau R_{x_1,x_2}(\tau) 
\end{eqnarray}

The benefits of calculating cross-correlation in the frequency domain as shown in equation~\ref{eqn:gcc2} are two folds. The first benefit is to speedup the calculation. Calculating cross-correlation using equation~\ref{eqn:gcc0} requires multiplying and summing the two signal vectors for each time shift $\tau$. With discrete signals of length $n$, it will take $O(n^2)$ number of calculations. Doing the same calculation in frequency domain, we need to transform the signal into frequency domain, multiple and sum the two transformed signal vectors once and then transform the result back to the time domain. Transforming a signal from time domain to frequency domain and back can be done efficiently with Fast Fourier Transform (FFT) and Inverse Fast Fourier Transform (IFFT), and the amount of calculation needed is $O(n\log n)$. Multiply and sum the transformed signal vectors takes another $O(n)$. Therefore, the total calculation required to calculate cross-correlation using Fourier transform is $O(n\log n)$, which is asymptotically faster than calculating in the time domain. This calculation speedup is especially beneficial in real-time interactive systems since significant time lag would produce unsmooth user experience.

The second benefit of formulating cross correlation in frequency domain is that it provides a unified framework to prefilter the signals. Cross-correlation with prefiltering is known as \emph{generalized cross correlation (GCC)}. Different prefiltering approaches have been investigated to improve arrival time difference estimation~\cite{tdoa:gcc1,tdoa:gcc2,tdoa:gcc3}. Under the GCC framework, the arrival time difference $t_0$ between two signals $x_1(t)$ and $x_2(t)$ is estimated as:
\begin{eqnarray} \label{eq:gcc}
t_0 &=& \arg\max_{\tau} R_{x_1x_2}(\tau) \\\label{eq:gcc2}
R_{x_1x_2}(\tau) &=& \int_{-\infty}^\infty W(\omega) X_1(\omega) X_2^{*}(\omega) e^{j\omega\tau} d\omega
\end{eqnarray}
$W(\omega)$ provides a way to prefilter signals passed to the cross correlation estimator. We focused on three ways of prefiltering the signal:
\begin{description}%[\IEEEsetlabelwidth{Very very long label}\IEEEusemathlabelsep]
\item[GCC] $W(\omega) = 1$. No prefiltering is done. This is unfiltered normal cross correlation.
\item[GCC\_PHAT] $W(\omega) = \frac{1}{\left|X_1(\omega)X_2^{*}(\omega)\right|}$. Each frequency is divided by its magnitude. Only phase information contributes to delay estimation.
\item[GCC\_PHAT\_SQRT] $W(\omega) = \frac{1}{\left|X_1(\omega)X_2^*(\omega)\right|^{0.5}}$. This is somewhere between GCC and GCC\_PHAT. Part of magnitude information is included in arrival time difference estimation.
\end{description}

To see the reasoning behind different prefiltering approaches, we separate the magnitude part from the phase part of $X_1(\omega)$ and $X_2(\omega)$ in Equation~\ref{eq:gcc2}:
\begin{eqnarray}
\label{eqn:phat1}
R_{x_1x_2}(\tau) &=& \int_{-\infty}^\infty W(\omega) |X_1(\omega)||X_2(\omega)| e^{j(\omega\tau - (\angle{X_2(\omega)} - \angle{X_1(\omega)}) } d\omega\\
&=& \int_{-\infty}^\infty \underbrace{W(w)|X_1(\omega)| |X_2(\omega)|}_{\mbox{weighting}} \cos(\Theta_\epsilon) d\omega
\end{eqnarray}
Where $\Theta_{e}$ is the phase error:
\[
\Theta_\epsilon = \omega\tau - (\angle X_2(\omega) - \angle X_1(\omega))
\]
We can look at the real part of equation~\ref{eqn:phat1} only since both $x_1(t)$ and $x_2(t)$ are real valued signals. When $\tau$ is the true arrival time difference between $x_1(t)$ and $x_2(t)$, phase error $\Theta_\epsilon  = 0$, and $\cos(\Theta_\epsilon) = 1$. When $\tau$ differs from the true arrival time difference, $\cos(\Theta_\epsilon) < 1$. Therefore, $\cos(\Theta_\epsilon)$ can be seen as a measure of the phase error, and $W(w)|X_1(\omega)||X_2(\omega)|$ describes how the error should be weighted at each frequency. The TDOA estimator essentially sums the weighted phase error at each frequency. 

Without any prefiltering (i.e $W(\omega)=1$), the estimator weighs the phase error at each frequency by the magnitude of the signal at that frequency. In this weighting scheme, phase error at frequencies with higher magnitudes are penalized more compared to frequencies with a lower magnitude. This weighting is appropriate if there is only one source present, since frequencies with higher magnitude have higher Signal to Noise Ratio (SNR). It makes sense to place higher weights at frequencies with higher SNR, since low SNR regions can be dominated by noise. 

However, with multiple sources, the source with the highest magnitude will dominate the phase error estimation, but there is no particular reason to assign a higher weight to the source with the highest volume. All sources should contribute equally in the phase error estimation. In GCC\_PHAT,  $W(\omega)$ is set to $\frac{1}{|X_1(\omega)||X_2(\omega)|}$. In effect it ignores the signal magnitude and weighs phase errors uniformly across frequencies. Since the phase error at every frequency is weighted equally, this technique will suffer from error accumulation if the source has a lot of low power regions in the frequency domain. This weighting is also beneficial if the source signal is white noise, since white noise should contain all frequency components with equal magnitude.

In GCC\_PHAT\_SQRT, $W(\omega)$ is set to $\frac{1}{(|X_1(\omega)||X_2(\omega)|)^{0.5}}$. Phase error weighting at each frequency still depends on the signal strength at that frequency, but the dependency is much weaker than that in unfiltered GCC. On the other hand, this weighting scheme doesn't go to the other extreme of completely ignoring signal strength information as does in GCC\_PHAT. This approach represents a balance between unfilterred GCC and GCC\_PHAT.

\section{Movement Tracking}
In the previous section, we discussed various ways of localizing acoustic source. For each received microphone data, the algorithm produces a point estimate of the source location. If we have multiple estimates of the source location, they can be intelligently combined to make better location prediction. 

In this section we describe two ways of combining past estimates to produce better estimates: Finite impulse response (FIR) filter and Kalman filter.

\subsection{FIR filter}
The impulse response is of finite duration for FIR filters. In our system, we only have access to location estimates from the past, we look at a special category of FIR filters: causal discrete-time FIR filters. In such systems, the output $y[n]$ is a linear weighted combination of past $N+1$ estimates from input $x[n]$:
\begin{eqnarray}
y[n] & = & b_0x[n] + b_1x[n-1] + \cdots + b_Nx[n-N]\\
& = & \sum_{i=0}^N b_i x[n-i]\\
\end{eqnarray}
where
\begin{itemize}
\item $y[n]$ is the output sequence
\item $x[n]$ is the input sequence
\item $N$ is the filter order
\item $b_i$ is the impulse response
\end{itemize}

By controlling the impulse response $b_i$, we specify how the past few data should be blended to produce the output. 

When $b_i=\frac{1}{N+1}$, each of the previous $N+1$ localization estimate contributes equally to the output. In this case, the filter becomes a simple averaging filter (also called rolling mean). If the source does not move during the past $N+1$ estimates, and assuming each estimate is an independent estimate of the true location with an additive Gaussian noise. Then it can be shown that the output after filter is an unbiased estimation of the true source location. However, if the source has moved during the past $N+1$ estimates, the filter would output the mean location for the past $N+1$ estimated locations, which results in a ``lagging'' effect between the true source location and the system output. After the stopped moving, the filter output catches up with the source location. 

To reduce the ``lagging'' effect ehxibited by the averaging filter, we can assign higher weights to more recent estimates. Recent estimates gets a higher contribution to the output location, which makes the filtered out tracks more closely with the sound source. However, if the localization system is noisy with large estimation variance, then the error in the most recent estimate dominates the filtered output and might produces a large error.

As a compromise between balancing the tradeoff between ``lagging'' problem from assigning equal weights to all estimates in the window and the error problem with assigning more weights. We will aim to design the system so that it produces estimates at a fast rate, so that the source would not move a large distance and it is safe to average the past a few estimates.

\subsection{Kalman filter}
The Kalman filter is a recursive filter where input data can be efficiently combined to produce online prediction. If all noise are Gaussian, the Kalman filter is a statistically optimum filter that minimizes squared error of the estimated parameters. Even if the noise is not Gaussian, given only the first two statistics of noise, Kalman filter is the best linear estimator[x]. Due to its statistical optimality and its recursive nature that enables online prediction, the Kalman filter has found applications in a wide range of areas. It has been used to track aircraft using RADAR, and to track Robot with sensors and beacons.

Kalman filter uses observed variables to infer hidden variables and use them to help predict the next state. In our system, observed variables $z$ are the $(x,y)$ coordinates of the localized acoustic source:
\[
z = \left[\begin{array}{c}
x\\
y\\
\end{array}\right]
\]
We can also model unobserved motion variables such as velocity $(\dot{x}, \dot{y})$ and acceleration $(\ddot{x}, \ddot{y})$. Then the state variables we are tracking $\mathbf{x}$ can be represented as:
\[
\mathbf{x} = \left[x, y, \dot{x}, \dot{y}, \ddot{x}, \ddot{y}\right]^T
\]
Note that by modeling up to acceleration, we are implicitly assuming higher order motion variables are constant (such as jerk). Kalman filter includes a process noise term $Q$ that can be used to partially reduce this effect, but there will exist systematic tracking bias there is acceleration change.

At any time instant, Kalman filter can use current state information to infer the predicted next state $\mathbf{x}^-$ and the predicted uncertainty $P^-$:
\begin{eqnarray}
\mathbf{x}^- & = &F \mathbf{x} + B_ku_k\\
P^- & = & FPF^T + Q\\
\end{eqnarray}
where
\begin{itemize}
\item F is the state transition matrix. In our system, the next coordinate output can be computed with laws of physics using current position, velocity and acceleration:
\[
F  =  \left(\begin{array}{cccccc}
1 & 0 & \frac{1}{2} & 0 & \frac{1}{2} & 0\\
1 & 0 & \frac{1}{2} & 0 & \frac{1}{2} & 0 \\
\end{array}\right)
\]
\item $F_k \hat{x}_k + B_k u_k$ describes the station transition form time $k$ to $k+1$. The Kalman filter is a general framework which allows not only tracking but also controlling the object. I these situations $u_k$ is the control input and $B$ describes the control input model. For our application, we are only tracking and not controlling the movement, $B_ku_k = 0$.
\end{itemize}
and an update step:
\begin{eqnarray}
y_k & = & z_k - H_k\hat{x}_k^-\\
S_k & = & H_kP_k^-H_k^T + R_k \\
K_k & = & P_k^-H_k^TS_k^{-1}\\
\hat{x}_k & = & \hat{x}_k^- + K_ky\\
P_k & = & (I-K_kH_k)P_k^-\\
\end{eqnarray}
where:
\begin{itemize}
\item $x$ is the state variable
\item $P$ is the covariance on the state variable $x$
\item $z$ is the measurement.
\item $y$ is the residue in the measurement sapce. 
\item $H$ is the measurement function that transforms from state space to measurement space.
\item $R$ is the measurement noise matrix that models sensor noise as a covariance matrix
\item $Q$ models the process noise
\end{itemize}

In our system, 
